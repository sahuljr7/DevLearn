# Write a program for Map Reduce to analyze a dataset and understand the Map Reduce workflow with execution on the Hadoop cluster.

## Aim:
The aim of the MapReduce program is to analyze a dataset using the Hadoop framework. This analysis can involve counting occurrences, aggregating data, or finding insights in the dataset.

## Software Required:
1. Hadoop Framework:
   - Download and install Hadoop on your cluster. Make sure HDFS and YARN services are set up and running.
2. Text Editor or IDE:
   - Use a text editor like Vim or an IDE like Eclipse to write your MapReduce program.

## Experiment Description:
1. Define Input and Output:
   - Choose the dataset you want to analyze and store it in HDFS. Define input and output paths for your MapReduce job.
2. Write Map and Reduce Functions:
   - Create Map and Reduce functions in Java. Map processes input and emits intermediate pairs. Reduce takes these pairs, groups them, and performs analysis.
3. Configure Job:
   - Set input/output paths, specify Mapper and Reducer classes, and configure other settings.
4. Compile and Package:
   - Compile your Java code and package it into a JAR file for submission to the Hadoop cluster.
5. Submit Job to Hadoop Cluster:
   - Use 'hadoop jar' command to submit your MapReduce job. Hadoop will distribute tasks across cluster nodes.
6. Monitor and Analyze:
   - Track job progress using Hadoop's Job Tracker or Resource Manager interface. Retrieve output from HDFS when job completes.

## Relevance:
- Hadoop cluster with at least one master and two worker nodes.
- Basic knowledge of Java programming.
- A sample dataset for analysis.