# Write a program to configure a small Hadoop cluster with at least one master and two worker nodes.

## Experiment 1.1: Configuring a Small Hadoop Cluster

Welcome to setting up a small Hadoop cluster! You'll create a simple one with a main node and two worker nodes. This cluster helps process big data using MapReduce.

## Aim:
Make a small Hadoop cluster for storing and processing data. This allows handling big datasets faster by doing many tasks at once.

## Software Needed:
Hadoop Distributed File System (HDFS)

## Description:
HDFS is a special file system for storing big files across many nodes in the Hadoop cluster. It lets applications access data quickly and ensures data stays safe even if a node fails.

## Algorithm: Configuring a Small Hadoop Cluster

1. Define Cluster Configuration:
   - Specify the number of nodes.
   - Determine each node's role (e.g., Namenode, Datanode, ResourceManager, NodeManager).

2. Install Hadoop:
   - Download suitable Hadoop distribution.
   - Install Hadoop on all nodes.

3. Configure Hadoop Environment:
   - Set Hadoop environment variables (e.g., HADOOP_HOME, HADOOP_CONF_DIR) on each node.

4. Configure HDFS:
   - Edit hdfs-site.xml for parameters like replication factor and block size.
   - Specify Namenode and Datanode configurations in core-site.xml.

5. Format HDFS:
   - Run hadoop namenode -format on the Namenode to initialize HDFS.

6. Start HDFS Services:
   - Start HDFS services on Namenode and Datanodes.

7. Configure YARN:
   - Edit yarn-site.xml for ResourceManager and NodeManager configurations.

8. Start YARN Services:
   - Start YARN services on ResourceManager and NodeManagers.

9. Verify Cluster Configuration:
   - Check Hadoop web UIs (Namenode UI, ResourceManager UI) to ensure all services are running.